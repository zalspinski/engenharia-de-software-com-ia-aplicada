import tf from '@tensorflow/tfjs-node';

async function trainModel(inputXs, outputYs) {
    const model = tf.sequential()

    // Primeira camada da rede:
    // entrada de 7 posições  (idade normalizada + 3 cores + 3 localizações)

    // 80 neurônios = aqui coloquei tudo isso, pq tem pouca base de treino
    // quanto mais neurônios, mais complexidade a rede pode aprender
    // e consequentemente, mais processamento ela vai usar

    // A ReLU age como um filtro:
    // É como se ela deixasse somente os dados interessantes seguirem viagem na rede
    // Se a informação que chegou nesse neurônio é positiva, passa pra frente!
    // Se for zero ou negativa, pode jogar fora, não vai servir pra nada
    model.add(tf.layers.dense({ inputShape: [7], units: 80, activation: 'relu' }))

    // Saída: 3 neurônios
    // um para cada categoria (premium, medium, basic)

    // activation: softmax normaliza a saída em probabilidades
    model.add(tf.layers.dense({ units: 3, activation: 'softmax' }))

    // Compilando o modelo
    // optimizer Adam (Adaptative Moment Estimation)
    // é um treinador pessoal moderno para reder neurais:
    // ajusta os pesos de forma eficiente e inteligente
    // aprender com histórico de erros e acertos

    // loss: categoricalCrossentropy
    // Ele compara o que o modelo "acha" (os score de cada categoria)
    // com a resposta certa
    // a categoria premium será sempre [1, 0, 0]

    // quanto mais distante da previsão do modelo da resposta correta
    // maior o erro (loss)
    // Exemplo clássico: classificação de imagens, recomendação, categorização de
    // usuáio
    // qualquer coisa em que a resposta certa é "apenas uma entre várias possíveis"

    model.compile({ optimizer: 'adam', loss: 'categoricalCrossentropy', metrics: ['accuracy'] })

    // Treinamento do modelo
    
    await model.fit(inputXs, outputYs, {
        epochs: 100, // número de vezes que o modelo vai passar por todo o dataset
        shuffle: true, // embaralha os dados a cada época para evitar padrões
        verbose: 0, // 0 = sem logs, 1 = barra de progresso, 2 = uma linha por época,
        // callbacks: {
        //     onEpochEnd: (epoch, log) => console.log(`Epoch: ${epoch}: loss = ${log.loss}`)
        // }
    })

    return model
}

async function predict(model, inputTensor) {
    // transformar o array js para o tensor (tfjs)
    const tfInput = tf.tensor2d(inputTensor)

    // Faz a predição (output será um vetor de 3 probabilidades)
    const prediction = model.predict(tfInput)
    const predictionData = await prediction.array()

    return predictionData[0].map((prob, index) => ({ prob, index }))
}

// Exemplo de pessoas para treino (cada pessoa com idade, cor e localização)
// const pessoas = [
//     { nome: "Erick", idade: 30, cor: "azul", localizacao: "São Paulo" },
//     { nome: "Ana", idade: 25, cor: "vermelho", localizacao: "Rio" },
//     { nome: "Carlos", idade: 40, cor: "verde", localizacao: "Curitiba" }
// ];

// Vetores de entrada com valores já normalizados e one-hot encoded
// Ordem: [idade_normalizada, azul, vermelho, verde, São Paulo, Rio, Curitiba]
// const tensorPessoas = [
//     [0.33, 1, 0, 0, 1, 0, 0], // Erick
//     [0, 0, 1, 0, 0, 1, 0],    // Ana
//     [1, 0, 0, 1, 0, 0, 1]     // Carlos
// ]

// Usamos apenas os dados numéricos, como a rede neural só entende números.
// tensorPessoasNormalizado corresponde ao dataset de entrada do modelo.
const tensorPessoasNormalizado = [
    [0.33, 1, 0, 0, 1, 0, 0], // Erick
    [0, 0, 1, 0, 0, 1, 0],    // Ana
    [1, 0, 0, 1, 0, 0, 1]     // Carlos
]

// Labels das categorias a serem previstas (one-hot encoded)
// [premium, medium, basic]
const labelsNomes = ["premium", "medium", "basic"]; // Ordem dos labels
const tensorLabels = [
    [1, 0, 0], // premium - Erick
    [0, 1, 0], // medium - Ana
    [0, 0, 1]  // basic - Carlos
];

// Criamos tensores de entrada (xs) e saída (ys) para treinar o modelo
const inputXs = tf.tensor2d(tensorPessoasNormalizado)
const outputYs = tf.tensor2d(tensorLabels)

// inputXs.print();
// outputYs.print();

// quanto mais dado melhor!
// assim o algorítmo consegue entender melhor os padrões complexos
// dos dados
const model = await trainModel(inputXs, outputYs)

const pessoa = { nome: 'John Doe', idade: 28, cor: 'verde', localizacao: 'Curitiba' }

// Normalizando a idade da nova pessoa usando o mesmo padrão do treino
// Exemplo: idade_min = 25, idade_max = 40, então (28 - 25) / (40 - 25) = 0.2

const pessoaTensorNormalizado = [[
    0.2, // idade normalizada
    1,  // azul
    0,  // vermelho
    0,  // verde
    0,  // São Paulo
    1,  // Rio
    0   // Curitiba
]]

const predictions = await predict(model, pessoaTensorNormalizado)
const results = predictions
    .sort((a, b) => b.prob - a.prob)
    .map( p => `${labelsNomes[p.index]} (${(p.prob * 100).toFixed(2)}%)`)
    .join('\n')

console.log(results)